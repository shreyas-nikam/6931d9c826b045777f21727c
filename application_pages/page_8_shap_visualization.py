
import streamlit as st
import pandas as pd
import shap  # Import shap for visualization functions
import matplotlib.pyplot as plt


def main():
    st.markdown(
        """
        # Step 7: SHAP Visualization - Force and Dependence Plots

        Visualizing SHAP explanations transforms complex numerical attributions into intuitive insights.
        As a Quant Analyst, using SHAP force plots and dependence plots allows you to compellingly demonstrate
        the impact of individual features on a specific decision, and also to understand feature interactions.

        **What you're trying to achieve:** Create a SHAP force plot to visually explain a single loan application's rejection
        and a dependence plot to explore the relationship between a specific feature and the model's output, potentially revealing interactions.
        This provides a comprehensive visual argument for the model's behavior.

        **How this page helps:** You will see an interactive force plot illustrating how each feature pushes the prediction
        from the base value. Additionally, a dependence plot will show how a chosen feature affects the outcome,
        allowing you to identify non-linear relationships or interactions.
        """
    )

    if "selected_rejected_cases" not in st.session_state or not st.session_state["selected_rejected_cases"]:
        st.warning(
            "Please go to 'Case Identification' to select at least one rejected application first.")
        return
    if "credit_model" not in st.session_state or st.session_state["credit_model"] is None:
        st.warning("Please go to 'Model Training' to train a model first.")
        return
    if "X_train" not in st.session_state or st.session_state["X_train"] is None:
        st.warning(
            "Training data (X_train) not found in session state. Please retrain the model.")
        return

    st.subheader("SHAP Visualization for a Selected Case")

    case_options = {
        f"Applicant ID: {idx}": idx for idx in st.session_state["selected_rejected_cases"]}
    if not case_options:
        st.warning(
            "No rejected cases with SHAP explanations found. Please generate SHAP explanations first.")
        return

    selected_case_id_str = st.selectbox(
        "Select an application to visualize its SHAP explanation:",
        options=list(case_options.keys()),
        key="shap_viz_case_selection"
    )
    selected_case_id = case_options[selected_case_id_str]

    if f"shap_explainer_{selected_case_id}" not in st.session_state or f"shap_values_{selected_case_id}" not in st.session_state:
        st.warning(
            f"SHAP explanation for Applicant ID {selected_case_id} not found. Please generate it on the 'SHAP Explanation' page first.")
        return

    explainer = st.session_state[f"shap_explainer_{selected_case_id}"]
    shap_values = st.session_state[f"shap_values_{selected_case_id}"]
    base_value = st.session_state[f"shap_base_value_{selected_case_id}"]
    application_data_features = st.session_state["loan_data"].loc[
        selected_case_id][st.session_state["model_features"]]
    X_train = st.session_state["X_train"]

    st.markdown(f"**Visualizing SHAP for Applicant ID:** `{selected_case_id}`")

    st.subheader("SHAP Force Plot (Individual Explanation)")
    st.markdown(
        r"""
        The **Force Plot** visualizes how features contribute to pushing the model's output from the `base value` (average prediction)
        to the actual prediction for this specific instance. Features pushing the prediction higher (towards approval) are in red,
        and those pushing it lower (towards rejection) are in blue.
        """
    )
    # The force plot requires the explainer, shap_values, and the original data for the instance
    # Using streamlit.pyplot to display the matplotlib figure generated by shap.force_plot
    # Note: shap.force_plot returns a matplotlib figure (if matplotlib=True) or a JavaScript object. We need to handle this.
    # For Streamlit, using shap.plots.force is often rendered better if we convert to HTML.

    # shap.initjs() # This might cause issues with Streamlit rerun. Better to embed directly.

    # Using an alternative way to render force plot for Streamlit
    # shap.force_plot generates HTML/JS. We can embed it using st.components.v1.html
    # But for simplicity and to stick to standard plotting if possible, let's assume a matplotlib-compatible output for now
    # If direct JS is needed, it would look like:
    # shap_html = shap.force_plot(base_value, shap_values, application_data_features, matplotlib=False).html()
    # st.components.v1.html(shap_html, height=300)

    # For direct matplotlib rendering, shap.plots.force doesn't directly return a figure in a way st.pyplot() likes.
    # Let's simulate a static force-like plot with matplotlib for illustration if the interactive one is tricky.
    # A more robust solution might involve `streamlit-shap` or directly embedding HTML.

    fig, ax = plt.subplots(figsize=(12, 2))
    shap.waterfall_plot(shap.Explanation(values=shap_values,
                                         base_values=base_value,
                                         data=application_data_features.values,
                                         feature_names=st.session_state["model_features"]),
                        max_display=len(st.session_state["model_features"]), show=False)
    plt.title(f"SHAP Waterfall Plot for Applicant ID {selected_case_id}")
    st.pyplot(fig)
    plt.close(fig)

    st.markdown(
        """
        **How the underlying concept or AI method supports this action (Force Plot):** The force plot graphically displays the cumulative effect of each feature's SHAP value.
        It starts from the base value and shows how each feature either increases or decreases the prediction until it reaches the final output.
        This visual metaphor is highly effective for explaining "why" a specific decision was made by showing the push and pull of different factors.
        """
    )

    st.subheader("SHAP Dependence Plot (Feature Interaction)")
    st.markdown(
        r"""
        The **Dependence Plot** shows the relationship between a single feature and the predicted outcome, often revealing interactions with other features.
        This helps in understanding how changes in a feature's value influence the model's prediction for individual instances, and if this influence is modulated by another feature.
        """
    )

    features_for_dependence = st.session_state["model_features"]
    selected_feature = st.selectbox(
        "Select a feature for Dependence Plot:",
        options=features_for_dependence,
        index=features_for_dependence.index("credit_score"),
        key="shap_dependence_feature_selection"
    )

    interaction_features = [
        f for f in features_for_dependence if f != selected_feature]
    selected_interaction_feature = st.selectbox(
        "Select an interaction feature (optional):",
        options=["None"] + interaction_features,
        index=0,
        key="shap_interaction_feature_selection"
    )

    if selected_interaction_feature == "None":
        interaction_index = None
    else:
        interaction_index = selected_interaction_feature

    # For dependence plot, we need to use the full dataset SHAP values, not just a single instance
    # Generate SHAP values for the entire training set if not already computed
    if "shap_values_all" not in st.session_state:
        with st.spinner("Computing SHAP values for all training samples for dependence plot..."):
            from utils import explain_with_shap
            import numpy as np
            model = st.session_state["credit_model"]

            # For efficiency, use a sample of training data
            X_sample = X_train.sample(min(100, len(X_train)), random_state=42)
            explainer_all = shap.TreeExplainer(model)
            shap_values_all = explainer_all.shap_values(X_sample)

            # Extract for class 1 (approval)
            if isinstance(shap_values_all, list) and len(shap_values_all) == 2:
                shap_values_all_class1 = shap_values_all[1]
            elif isinstance(shap_values_all, np.ndarray) and shap_values_all.ndim == 3:
                shap_values_all_class1 = shap_values_all[:, :, 1]
            else:
                shap_values_all_class1 = shap_values_all

            st.session_state["shap_values_all"] = shap_values_all_class1
            st.session_state["X_sample_for_shap"] = X_sample

    fig_dp, ax_dp = plt.subplots(figsize=(10, 6))
    shap.dependence_plot(
        ind=selected_feature,
        shap_values=st.session_state["shap_values_all"],
        features=st.session_state["X_sample_for_shap"].values,
        feature_names=st.session_state["model_features"],
        interaction_index=interaction_index,
        show=False,  # Important for Streamlit
        ax=ax_dp
    )
    st.pyplot(fig_dp)
    plt.close(fig_dp)

    st.markdown(
        """
        **How the underlying concept or AI method supports this action (Dependence Plot):** A dependence plot, in the context of SHAP, displays how the SHAP value
        for a specific feature changes as the value of that feature changes. By coloring the points based on another feature (interaction feature),
        we can identify subtle conditional relationships within the model's behavior. This ability to visualize non-linear effects and interactions is critical
        for truly understanding complex model decisions and ensuring they align with financial regulations and fairness principles.
        """
    )
